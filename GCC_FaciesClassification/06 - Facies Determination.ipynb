{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Facies Classifier\n",
    "\n",
    "George Crowther\n",
    "\n",
    "This is an extension / amalgamation of prior entries. The workflow remains not dissimilar to those completed previously, this is:\n",
    "- Load and set strings to integers\n",
    "- Cursory data examination, this workbook does not attempt to detail the full data analysis\n",
    "- Group data by well and brute force feature creation\n",
    "    - Feature creation focuses on bringing results from adjacent samples into features\n",
    "    - Look at some ratios between features\n",
    "- Leaving out two wells at a time, use TPOT to generate a pipeline for prediction.\n",
    "- Modal vote on fitted model predicting on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slygeorge/anaconda3/envs/seg_competition/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"35a2b57c-3705-4213-842d-cb769d1c8063\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"35a2b57c-3705-4213-842d-cb769d1c8063\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"35a2b57c-3705-4213-842d-cb769d1c8063\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '35a2b57c-3705-4213-842d-cb769d1c8063' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"35a2b57c-3705-4213-842d-cb769d1c8063\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"35a2b57c-3705-4213-842d-cb769d1c8063\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bokeh.plotting as bk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "\n",
    "import sys\n",
    "sys.path.append('~/home/slygeorge/Documents/Python/SEG ML Competition')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "bk.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFwe, SelectKBest, f_classif, SelectPercentile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, Binarizer, Normalizer, StandardScaler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    XGBClassifier(learning_rate=0.02, max_depth=5, min_child_weight=20, n_estimators=500, subsample=0.19)\n",
    "),\n",
    "    make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", LogisticRegression(C=0.13, dual=False, penalty=\"l1\"))]), FunctionTransformer(lambda X: X)),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    Binarizer(threshold=0.72),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", GradientBoostingClassifier(learning_rate=1.0, max_features=1.0, n_estimators=500))]), FunctionTransformer(lambda X: X)),\n",
    "    BernoulliNB(alpha=28.0, binarize=0.85, fit_prior=True)\n",
    "),\n",
    "    make_pipeline(\n",
    "    Normalizer(norm=\"l1\"),\n",
    "    make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),\n",
    "    SelectKBest(k=47, score_func=f_classif),\n",
    "    SelectFwe(alpha=0.05, score_func=f_classif),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", LinearSVC(C=0.26, dual=False, penalty=\"l2\"))]), FunctionTransformer(lambda X: X)),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    Normalizer(norm=\"l2\"),\n",
    "    make_union(VotingClassifier([(\"est\", ExtraTreesClassifier(criterion=\"entropy\", max_features=0.3, n_estimators=500))]), FunctionTransformer(lambda X: X)),\n",
    "    GaussianNB()\n",
    "),\n",
    "    make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", BernoulliNB(alpha=49.0, binarize=0.06, fit_prior=True))]), FunctionTransformer(lambda X: X)),\n",
    "    StandardScaler(),\n",
    "    make_union(VotingClassifier([(\"est\", GradientBoostingClassifier(learning_rate=0.87, max_features=0.87, n_estimators=500))]), FunctionTransformer(lambda X: X)),\n",
    "    ExtraTreesClassifier(criterion=\"entropy\", max_features=0.001, n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),\n",
    "    BernoulliNB(alpha=1e-06, binarize=0.09, fit_prior=True)\n",
    "),\n",
    "    make_pipeline(\n",
    "    Normalizer(norm=\"max\"),\n",
    "    MinMaxScaler(),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    SelectPercentile(percentile=18, score_func=f_classif),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    SelectKBest(k=50, score_func=f_classif),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    XGBClassifier(learning_rate=0.51, max_depth=10, min_child_weight=20, n_estimators=500, subsample=1.0)\n",
    "),\n",
    "    make_pipeline(\n",
    "    make_union(VotingClassifier([(\"est\", KNeighborsClassifier(n_neighbors=5, weights=\"uniform\"))]), FunctionTransformer(lambda X: X)),\n",
    "    RandomForestClassifier(n_estimators=500)\n",
    "),\n",
    "    make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SelectPercentile(percentile=19, score_func=f_classif),\n",
    "    LinearSVC(C=0.02, dual=False, penalty=\"l1\")\n",
    "),\n",
    "    make_pipeline(\n",
    "    XGBClassifier(learning_rate=0.01, max_depth=10, min_child_weight=20, n_estimators=500, subsample=0.36)\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_path = '../training_data.csv'\n",
    "test_path = '../validation_data_nofacies.csv'\n",
    "\n",
    "facies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "\n",
    "\n",
    "def feature_extraction(file_path, retain_class = True):\n",
    "\n",
    "    # Read training data to dataframe\n",
    "    test = pd.read_csv(file_path)\n",
    "    \n",
    "    if 'Facies' in test.columns:\n",
    "        test.rename(columns={'Facies': 'class'}, inplace=True)\n",
    "\n",
    "    # Set string features to integers\n",
    "\n",
    "    for i, value in enumerate(test['Formation'].unique()):\n",
    "        test.loc[test['Formation'] == value, 'Formation'] = i\n",
    "\n",
    "    for i, value in enumerate(test['Well Name'].unique()):\n",
    "        test.loc[test['Well Name'] == value, 'Well Name'] = i\n",
    "\n",
    "    # The first thing that will be done is to upsample and interpolate the training data,\n",
    "    # the objective here is to provide significantly more samples to train the regressor on and\n",
    "    # also to capture more of the sample interdependancy.\n",
    "    upsampled_arrays = []\n",
    "    test['orig_index'] = test.index\n",
    "\n",
    "    # Use rolling windows through upsampled frame, grouping by well name.\n",
    "\n",
    "    # Empty list to hold frames\n",
    "    mean_frames = []\n",
    "    above = []\n",
    "    below = []\n",
    "\n",
    "    for well, group in test.groupby('Well Name'):\n",
    "        # Empty list to hold rolling frames\n",
    "        constructor_list = []\n",
    "        for f in resample_factors:\n",
    "\n",
    "            working_frame = group[['Depth', 'GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M',\n",
    "           'RELPOS']]\n",
    "\n",
    "            mean_frame = working_frame.rolling(window = f, center = True).mean().interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            mean_frame.columns = ['Mean_{0}_{1}'.format(f, column) for column in mean_frame.columns]\n",
    "            max_frame = working_frame.rolling(window = f, center = True).max().interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            max_frame.columns = ['Max_{0}_{1}'.format(f, column) for column in max_frame.columns]\n",
    "            min_frame = working_frame.rolling(window = f, center = True).min().interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            min_frame.columns = ['Min_{0}_{1}'.format(f, column) for column in min_frame.columns]\n",
    "            std_frame = working_frame.rolling(window = f, center = True).std().interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            std_frame.columns = ['Std_{0}_{1}'.format(f, column) for column in std_frame.columns]\n",
    "            var_frame = working_frame.rolling(window = f, center = True).var().interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            var_frame.columns = ['Var_{0}_{1}'.format(f, column) for column in var_frame.columns]\n",
    "            diff_frame = working_frame.diff(f, axis = 0).interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            diff_frame.columns = ['Diff_{0}_{1}'.format(f, column) for column in diff_frame.columns]\n",
    "            rdiff_frame = working_frame.sort_index(ascending = False).diff(f, axis = 0).interpolate(method = 'index', limit_direction = 'both', limit = None).sort_index()\n",
    "            rdiff_frame.columns = ['Rdiff_{0}_{1}'.format(f, column) for column in rdiff_frame.columns]\n",
    "            skew_frame = working_frame.rolling(window = f, center = True).skew().interpolate(method = 'index', limit_direction = 'both', limit = None)\n",
    "            skew_frame.columns = ['Skew_{0}_{1}'.format(f, column) for column in skew_frame.columns]\n",
    "\n",
    "            f_frame = pd.concat((mean_frame, max_frame, min_frame, std_frame, var_frame, diff_frame, rdiff_frame), axis = 1)\n",
    "\n",
    "            constructor_list.append(f_frame)\n",
    "\n",
    "        well_frame = pd.concat(constructor_list, axis = 1)\n",
    "        well_frame['Well Name'] = well\n",
    "        # orig index is holding the original index locations, to make extracting the results trivial\n",
    "        well_frame['orig_index'] = group['orig_index']\n",
    "        df = group.sort_values('Depth')\n",
    "        u = df.shift(-1).fillna(method = 'ffill')\n",
    "        b = df.shift(1).fillna(method = 'bfill')\n",
    "        above.append(u[div_columns])\n",
    "        below.append(b[div_columns])\n",
    "\n",
    "        mean_frames.append(well_frame.fillna(method = 'bfill').fillna(method = 'ffill'))\n",
    "\n",
    "    frame = test\n",
    "    frame.index = frame['orig_index']\n",
    "    frame.drop(['orig_index', 'Well Name'], axis = 1, inplace = True)\n",
    "\n",
    "    for f in mean_frames:\n",
    "        f.index = f['orig_index']\n",
    "\n",
    "    rolling_frame = pd.concat(mean_frames, axis = 0)\n",
    "    above_frame = pd.concat(above)\n",
    "    above_frame.columns = ['above_'+ column for column in above_frame.columns]\n",
    "    below_frame = pd.concat(below)\n",
    "    below_frame.columns = ['below_'+ column for column in below_frame.columns]\n",
    "    upsampled_frame = pd.concat((frame, rolling_frame, above_frame, below_frame), axis = 1)\n",
    "\n",
    "    features = [feature for feature in upsampled_frame.columns if 'class' not in feature]\n",
    "\n",
    "    std_scaler = preprocessing.StandardScaler().fit(upsampled_frame[features])\n",
    "    train_std = std_scaler.transform(upsampled_frame[features])\n",
    "\n",
    "    train_std_frame = upsampled_frame\n",
    "    for i, column in enumerate(features):\n",
    "        train_std_frame.loc[:, column] = train_std[:, i]\n",
    "\n",
    "    upsampled_frame_std = train_std_frame\n",
    "\n",
    "    for feature in div_columns:\n",
    "        for f in div_columns:\n",
    "            if f == feature:\n",
    "                continue\n",
    "            upsampled_frame['{0}_{1}'.format(feature, f)] = upsampled_frame[f] / upsampled_frame[feature]\n",
    " \n",
    "    return upsampled_frame_std, features\n",
    "\n",
    "train_data_set, features = feature_extraction(train_path)\n",
    "test_data_set, test_features = feature_extraction(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Formation</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "      <th>...</th>\n",
       "      <th>NM_M_PHIND</th>\n",
       "      <th>NM_M_PE</th>\n",
       "      <th>NM_M_RELPOS</th>\n",
       "      <th>RELPOS_Depth</th>\n",
       "      <th>RELPOS_GR</th>\n",
       "      <th>RELPOS_ILD_log10</th>\n",
       "      <th>RELPOS_DeltaPHI</th>\n",
       "      <th>RELPOS_PHIND</th>\n",
       "      <th>RELPOS_PE</th>\n",
       "      <th>RELPOS_NM_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.346013</td>\n",
       "      <td>-0.632316</td>\n",
       "      <td>0.366749</td>\n",
       "      <td>0.088008</td>\n",
       "      <td>1.212737</td>\n",
       "      <td>-0.203723</td>\n",
       "      <td>0.976532</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>1.672943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204354</td>\n",
       "      <td>-0.979558</td>\n",
       "      <td>-1.678127</td>\n",
       "      <td>-0.377966</td>\n",
       "      <td>0.219224</td>\n",
       "      <td>0.052606</td>\n",
       "      <td>0.724912</td>\n",
       "      <td>-0.121775</td>\n",
       "      <td>0.583721</td>\n",
       "      <td>-0.595902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.346013</td>\n",
       "      <td>-0.628499</td>\n",
       "      <td>0.393005</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>2.035209</td>\n",
       "      <td>-0.119283</td>\n",
       "      <td>0.418505</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>1.599708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119652</td>\n",
       "      <td>-0.419802</td>\n",
       "      <td>-1.604665</td>\n",
       "      <td>-0.392884</td>\n",
       "      <td>0.245673</td>\n",
       "      <td>0.047259</td>\n",
       "      <td>1.272238</td>\n",
       "      <td>-0.074565</td>\n",
       "      <td>0.261613</td>\n",
       "      <td>-0.623183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.346013</td>\n",
       "      <td>-0.624682</td>\n",
       "      <td>0.418613</td>\n",
       "      <td>0.063194</td>\n",
       "      <td>2.149973</td>\n",
       "      <td>-0.056278</td>\n",
       "      <td>-0.139522</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>1.522985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.139955</td>\n",
       "      <td>-1.527705</td>\n",
       "      <td>-0.410169</td>\n",
       "      <td>0.274863</td>\n",
       "      <td>0.041494</td>\n",
       "      <td>1.411683</td>\n",
       "      <td>-0.036952</td>\n",
       "      <td>-0.091611</td>\n",
       "      <td>-0.654577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.346013</td>\n",
       "      <td>-0.620865</td>\n",
       "      <td>0.647138</td>\n",
       "      <td>0.050788</td>\n",
       "      <td>1.977828</td>\n",
       "      <td>-0.047834</td>\n",
       "      <td>-0.251128</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>1.449750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047982</td>\n",
       "      <td>0.251906</td>\n",
       "      <td>-1.454243</td>\n",
       "      <td>-0.428256</td>\n",
       "      <td>0.446379</td>\n",
       "      <td>0.035032</td>\n",
       "      <td>1.364254</td>\n",
       "      <td>-0.032994</td>\n",
       "      <td>-0.173221</td>\n",
       "      <td>-0.687643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.346013</td>\n",
       "      <td>-0.617047</td>\n",
       "      <td>0.273719</td>\n",
       "      <td>0.017704</td>\n",
       "      <td>1.901319</td>\n",
       "      <td>-0.023801</td>\n",
       "      <td>-0.362733</td>\n",
       "      <td>-0.996911</td>\n",
       "      <td>1.376515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>0.363857</td>\n",
       "      <td>-1.380781</td>\n",
       "      <td>-0.448268</td>\n",
       "      <td>0.198849</td>\n",
       "      <td>0.012861</td>\n",
       "      <td>1.381255</td>\n",
       "      <td>-0.017291</td>\n",
       "      <td>-0.263516</td>\n",
       "      <td>-0.724228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  Formation     Depth        GR  ILD_log10  DeltaPHI     PHIND  \\\n",
       "0      3  -1.346013 -0.632316  0.366749   0.088008  1.212737 -0.203723   \n",
       "1      3  -1.346013 -0.628499  0.393005   0.075601  2.035209 -0.119283   \n",
       "2      3  -1.346013 -0.624682  0.418613   0.063194  2.149973 -0.056278   \n",
       "3      3  -1.346013 -0.620865  0.647138   0.050788  1.977828 -0.047834   \n",
       "4      3  -1.346013 -0.617047  0.273719   0.017704  1.901319 -0.023801   \n",
       "\n",
       "         PE      NM_M    RELPOS     ...       NM_M_PHIND   NM_M_PE  \\\n",
       "0  0.976532 -0.996911  1.672943     ...         0.204354 -0.979558   \n",
       "1  0.418505 -0.996911  1.599708     ...         0.119652 -0.419802   \n",
       "2 -0.139522 -0.996911  1.522985     ...         0.056452  0.139955   \n",
       "3 -0.251128 -0.996911  1.449750     ...         0.047982  0.251906   \n",
       "4 -0.362733 -0.996911  1.376515     ...         0.023875  0.363857   \n",
       "\n",
       "   NM_M_RELPOS  RELPOS_Depth  RELPOS_GR  RELPOS_ILD_log10  RELPOS_DeltaPHI  \\\n",
       "0    -1.678127     -0.377966   0.219224          0.052606         0.724912   \n",
       "1    -1.604665     -0.392884   0.245673          0.047259         1.272238   \n",
       "2    -1.527705     -0.410169   0.274863          0.041494         1.411683   \n",
       "3    -1.454243     -0.428256   0.446379          0.035032         1.364254   \n",
       "4    -1.380781     -0.448268   0.198849          0.012861         1.381255   \n",
       "\n",
       "   RELPOS_PHIND  RELPOS_PE  RELPOS_NM_M  \n",
       "0     -0.121775   0.583721    -0.595902  \n",
       "1     -0.074565   0.261613    -0.623183  \n",
       "2     -0.036952  -0.091611    -0.654577  \n",
       "3     -0.032994  -0.173221    -0.687643  \n",
       "4     -0.017291  -0.263516    -0.724228  \n",
       "\n",
       "[5 rows x 364 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0\n",
      "    training:   [-0.53211804 -0.10387962  0.3243588   0.75259723  1.18083565  1.60907407]\n",
      "    validation: [-1.38859488 -0.96035646]\n",
      "Split 1\n",
      "    training:   [-0.96035646 -0.53211804  0.3243588   0.75259723  1.18083565  1.60907407]\n",
      "    validation: [-1.38859488 -0.10387962]\n",
      "Split 2\n",
      "    training:   [-0.96035646 -0.53211804 -0.10387962  0.75259723  1.18083565  1.60907407]\n",
      "    validation: [-1.38859488  0.3243588 ]\n",
      "Split 3\n",
      "    training:   [-0.96035646 -0.53211804 -0.10387962  0.3243588   0.75259723  1.18083565]\n",
      "    validation: [-1.38859488  1.60907407]\n",
      "Split 4\n",
      "    training:   [-1.38859488 -0.53211804 -0.10387962  0.3243588   1.18083565  1.60907407]\n",
      "    validation: [-0.96035646  0.75259723]\n",
      "Split 5\n",
      "    training:   [-1.38859488 -0.53211804 -0.10387962  0.3243588   0.75259723  1.60907407]\n",
      "    validation: [-0.96035646  1.18083565]\n",
      "Split 6\n",
      "    training:   [-1.38859488 -0.53211804 -0.10387962  0.3243588   0.75259723  1.18083565]\n",
      "    validation: [-0.96035646  1.60907407]\n",
      "Split 7\n",
      "    training:   [-1.38859488 -0.96035646 -0.10387962  0.3243588   0.75259723  1.18083565]\n",
      "    validation: [-0.53211804  1.60907407]\n",
      "Split 8\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804  0.3243588   1.18083565  1.60907407]\n",
      "    validation: [-0.10387962  0.75259723]\n",
      "Split 9\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804  0.3243588   0.75259723  1.60907407]\n",
      "    validation: [-0.10387962  1.18083565]\n",
      "Split 10\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804  0.3243588   0.75259723  1.18083565]\n",
      "    validation: [-0.10387962  1.60907407]\n",
      "Split 11\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804 -0.10387962  1.18083565  1.60907407]\n",
      "    validation: [ 0.3243588   0.75259723]\n",
      "Split 12\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804 -0.10387962  0.75259723  1.60907407]\n",
      "    validation: [ 0.3243588   1.18083565]\n",
      "Split 13\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804 -0.10387962  0.75259723  1.18083565]\n",
      "    validation: [ 0.3243588   1.60907407]\n",
      "Split 14\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804 -0.10387962  0.3243588   1.18083565]\n",
      "    validation: [ 0.75259723  1.60907407]\n",
      "Split 15\n",
      "    training:   [-1.38859488 -0.96035646 -0.53211804 -0.10387962  0.3243588   0.75259723]\n",
      "    validation: [ 1.18083565  1.60907407]\n"
     ]
    }
   ],
   "source": [
    "lpgo = LeavePGroupsOut(2)\n",
    "\n",
    "split_list = []\n",
    "fitted_models = []\n",
    "\n",
    "for train, val in lpgo.split(train_data_set[features], \n",
    "                             train_data_set['class'], \n",
    "                             groups = train_data_set['Well Name']):\n",
    "    hist_tr = np.histogram(train_data_set.loc[train, 'class'], \n",
    "                           bins = np.arange(len(facies_labels) + 1) + 0.5)\n",
    "    hist_val = np.histogram(train_data_set.loc[val, 'class'],\n",
    "                           bins = np.arange(len(facies_labels) + 1) + 0.5)\n",
    "    if np.all(hist_tr[0] != 0) & np.all(hist_val[0] != 0):\n",
    "        split_list.append({'train': train, 'val': val})\n",
    "        \n",
    "for s, split in enumerate(split_list):\n",
    "    print('Split %d' % s)\n",
    "    print('    training:   %s' % (train_data_set['Well Name'].loc[split['train']].unique()))\n",
    "    print('    validation: %s' % (train_data_set['Well Name'].loc[split['val']].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slygeorge/anaconda3/envs/seg_competition/lib/python3.5/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "fitted_models = []\n",
    "r = []\n",
    "\n",
    "for i, split in enumerate(split_list):\n",
    "\n",
    "    # Select training and validation data from current split\n",
    "    X_tr = train_data_set.loc[split['train'], features]\n",
    "    X_v = train_data_set.loc[split['val'], features]\n",
    "    y_tr = train_data_set.loc[split['train'], 'class']\n",
    "    y_v = train_data_set.loc[split['val'], 'class']\n",
    "\n",
    "    # Fit model from split\n",
    "    fitted_models.append(models[i].fit(X_tr, y_tr))\n",
    "    \n",
    "    # Predict for model\n",
    "    r.append(fitted_models[-1].predict(test_data_set[test_features]))\n",
    "    \n",
    "results = mode(np.vstack(r))[0][0]\n",
    "\n",
    "test_data_set['Facies'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>RELPOS_NM_M</th>\n",
       "      <th>RELPOS_PE</th>\n",
       "      <th>RELPOS_PHIND</th>\n",
       "      <th>RELPOS_DeltaPHI</th>\n",
       "      <th>RELPOS_ILD_log10</th>\n",
       "      <th>RELPOS_GR</th>\n",
       "      <th>RELPOS_Depth</th>\n",
       "      <th>NM_M_RELPOS</th>\n",
       "      <th>NM_M_PE</th>\n",
       "      <th>...</th>\n",
       "      <th>Mean_2_Depth</th>\n",
       "      <th>RELPOS</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>PE</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>GR</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Formation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.884952</td>\n",
       "      <td>-0.059289</td>\n",
       "      <td>-0.118109</td>\n",
       "      <td>0.079374</td>\n",
       "      <td>-0.076787</td>\n",
       "      <td>0.191931</td>\n",
       "      <td>-1.156836</td>\n",
       "      <td>-1.130005</td>\n",
       "      <td>0.066997</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.892984</td>\n",
       "      <td>1.640888</td>\n",
       "      <td>-1.452107</td>\n",
       "      <td>-0.097287</td>\n",
       "      <td>-0.193803</td>\n",
       "      <td>0.130243</td>\n",
       "      <td>-0.125999</td>\n",
       "      <td>0.314937</td>\n",
       "      <td>-1.898239</td>\n",
       "      <td>-1.351854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.928980</td>\n",
       "      <td>-0.308522</td>\n",
       "      <td>0.036349</td>\n",
       "      <td>0.678435</td>\n",
       "      <td>-0.180501</td>\n",
       "      <td>0.456716</td>\n",
       "      <td>-1.211001</td>\n",
       "      <td>-1.076450</td>\n",
       "      <td>0.332108</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.892984</td>\n",
       "      <td>1.563120</td>\n",
       "      <td>-1.452107</td>\n",
       "      <td>-0.482257</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>1.060476</td>\n",
       "      <td>-0.282144</td>\n",
       "      <td>0.713902</td>\n",
       "      <td>-1.892939</td>\n",
       "      <td>-1.351854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.977618</td>\n",
       "      <td>-0.611844</td>\n",
       "      <td>0.252408</td>\n",
       "      <td>1.281514</td>\n",
       "      <td>-0.234336</td>\n",
       "      <td>0.618819</td>\n",
       "      <td>-1.270837</td>\n",
       "      <td>-1.022894</td>\n",
       "      <td>0.625852</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.887683</td>\n",
       "      <td>1.485351</td>\n",
       "      <td>-1.452107</td>\n",
       "      <td>-0.908803</td>\n",
       "      <td>0.374915</td>\n",
       "      <td>1.903499</td>\n",
       "      <td>-0.348072</td>\n",
       "      <td>0.919164</td>\n",
       "      <td>-1.887639</td>\n",
       "      <td>-1.351854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.034229</td>\n",
       "      <td>-0.742690</td>\n",
       "      <td>0.218967</td>\n",
       "      <td>1.376426</td>\n",
       "      <td>-0.181180</td>\n",
       "      <td>0.596973</td>\n",
       "      <td>-1.340651</td>\n",
       "      <td>-0.966904</td>\n",
       "      <td>0.718110</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.882383</td>\n",
       "      <td>1.404048</td>\n",
       "      <td>-1.452107</td>\n",
       "      <td>-1.042773</td>\n",
       "      <td>0.307440</td>\n",
       "      <td>1.932569</td>\n",
       "      <td>-0.254385</td>\n",
       "      <td>0.838179</td>\n",
       "      <td>-1.882338</td>\n",
       "      <td>-1.351854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.094872</td>\n",
       "      <td>-0.736314</td>\n",
       "      <td>0.100984</td>\n",
       "      <td>1.281789</td>\n",
       "      <td>-0.074072</td>\n",
       "      <td>0.503166</td>\n",
       "      <td>-1.415266</td>\n",
       "      <td>-0.913348</td>\n",
       "      <td>0.672511</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877082</td>\n",
       "      <td>1.326280</td>\n",
       "      <td>-1.452107</td>\n",
       "      <td>-0.976558</td>\n",
       "      <td>0.133933</td>\n",
       "      <td>1.700011</td>\n",
       "      <td>-0.098240</td>\n",
       "      <td>0.667339</td>\n",
       "      <td>-1.877038</td>\n",
       "      <td>-1.351854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 364 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Facies  RELPOS_NM_M  RELPOS_PE  RELPOS_PHIND  RELPOS_DeltaPHI  \\\n",
       "orig_index                                                                  \n",
       "0                2    -0.884952  -0.059289     -0.118109         0.079374   \n",
       "1                2    -0.928980  -0.308522      0.036349         0.678435   \n",
       "2                2    -0.977618  -0.611844      0.252408         1.281514   \n",
       "3                2    -1.034229  -0.742690      0.218967         1.376426   \n",
       "4                2    -1.094872  -0.736314      0.100984         1.281789   \n",
       "\n",
       "            RELPOS_ILD_log10  RELPOS_GR  RELPOS_Depth  NM_M_RELPOS   NM_M_PE  \\\n",
       "orig_index                                                                     \n",
       "0                  -0.076787   0.191931     -1.156836    -1.130005  0.066997   \n",
       "1                  -0.180501   0.456716     -1.211001    -1.076450  0.332108   \n",
       "2                  -0.234336   0.618819     -1.270837    -1.022894  0.625852   \n",
       "3                  -0.181180   0.596973     -1.340651    -0.966904  0.718110   \n",
       "4                  -0.074072   0.503166     -1.415266    -0.913348  0.672511   \n",
       "\n",
       "              ...      Mean_2_Depth    RELPOS      NM_M        PE     PHIND  \\\n",
       "orig_index    ...                                                             \n",
       "0             ...         -1.892984  1.640888 -1.452107 -0.097287 -0.193803   \n",
       "1             ...         -1.892984  1.563120 -1.452107 -0.482257  0.056818   \n",
       "2             ...         -1.887683  1.485351 -1.452107 -0.908803  0.374915   \n",
       "3             ...         -1.882383  1.404048 -1.452107 -1.042773  0.307440   \n",
       "4             ...         -1.877082  1.326280 -1.452107 -0.976558  0.133933   \n",
       "\n",
       "            DeltaPHI  ILD_log10        GR     Depth  Formation  \n",
       "orig_index                                                      \n",
       "0           0.130243  -0.125999  0.314937 -1.898239  -1.351854  \n",
       "1           1.060476  -0.282144  0.713902 -1.892939  -1.351854  \n",
       "2           1.903499  -0.348072  0.919164 -1.887639  -1.351854  \n",
       "3           1.932569  -0.254385  0.838179 -1.882338  -1.351854  \n",
       "4           1.700011  -0.098240  0.667339 -1.877038  -1.351854  \n",
       "\n",
       "[5 rows x 364 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_set.iloc[:, ::-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_set.iloc[:, ::-1].to_csv('06 - Combined Models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:seg_competition]",
   "language": "python",
   "name": "conda-env-seg_competition-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
